{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan12Dubey/Titanic-Classification/blob/main/Titanic_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw4QHJlfsSF4"
      },
      "source": [
        "## Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of the mini-hackathon you will be able to:\n",
        "* Perform Data preprocessing\n",
        "* Apply different ML algorithms on the **Titanic** dataset\n",
        "* Perform VotingClassifier\n",
        "* Able to participate and submit predictions in the Kaggle competition"
      ],
      "metadata": {
        "id": "EsSB95vPsoxS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh70dVHx0G_B"
      },
      "source": [
        "## Dataset Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-GMJTRb0Iyy"
      },
      "source": [
        "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
        "\n",
        "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of many passengers and crew.\n",
        "\n",
        "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
        "\n",
        "[ Data Set Link: Kaggle competition](https://www.kaggle.com/competitions/titanic)\n",
        "\n",
        "<br/>\n",
        "\n",
        "### Data Set Characteristics:\n",
        "\n",
        "**PassengerId:** Id of the Passenger\n",
        "\n",
        "**Survived:** Survived or Not information\n",
        "\n",
        "**Pclass:** Socio-economic status (SES)\n",
        "  * 1st = Upper\n",
        "  * 2nd = Middle\n",
        "  * 3rd = Lower\n",
        "\n",
        "**Name:** Surname, First Names of the Passenger\n",
        "\n",
        "**Sex:** Gender of the Passenger\n",
        "\n",
        "**Age:** Age of the Passenger\n",
        "\n",
        "**SibSp:**\tNo. of siblings/spouse of the passenger aboard the Titanic\n",
        "\n",
        "**Parch:**\tNo. of parents/children of the passenger aboard the Titanic\n",
        "\n",
        "**Ticket:**\tTicket number\n",
        "\n",
        "**Fare:** Passenger fare\n",
        "\n",
        "**Cabin:**\tCabin number\n",
        "\n",
        "**Embarked:** Port of Embarkation\n",
        "  * S = Southampton\n",
        "  * C = Cherbourg\n",
        "  * Q = Queenstown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "Build a predictive model that answers the question: “what sort of people were more likely to survive?” using titanic's passenger data (ie name, age, gender, socio-economic class, etc)."
      ],
      "metadata": {
        "id": "KmusUbENKSEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download the datasets\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook=\"U1_MH1_Data_Munging\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/titanic.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/test_titanic.csv\")\n",
        "    print(\"Data downloaded successfully\")\n",
        "    return\n",
        "\n",
        "setup()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ViFc50xKK-tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "RM8x-pMDLQuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Use **titanic.csv** for training & testing purpose and **test_titanic.csv** for submitting the prediction on Kaggle competition."
      ],
      "metadata": {
        "id": "RKzNj9hyNNPJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fBLdQaJtd4"
      },
      "source": [
        "## Exercise 1 - Load and Explore the Data\n",
        "\n",
        "* Understand different features in the training dataset\n",
        "* Understand the data types of each column\n",
        "* Notice the columns of missing values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6HQH7abkyL"
      },
      "source": [
        "#### Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "SwWXBsB5MJhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = pd.read_csv('titanic.csv')\n",
        "Data.head()"
      ],
      "metadata": {
        "id": "4tUzpuurwCyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ktmr8lh8AS"
      },
      "source": [
        "Data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 02: Split the data into train and test sets\n",
        "Note: Apply all your data preprocessing steps in the train set first and keep the test set aside."
      ],
      "metadata": {
        "id": "eQGya6YLOku-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = Data['Survived']\n",
        "feature = Data.drop(['Survived'],axis=1)"
      ],
      "metadata": {
        "id": "an__PqiLKMVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(feature,label,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "mps-O7zbPPcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "metadata": {
        "id": "XT1GQf_E4dA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pMXWDyaspe3"
      },
      "source": [
        "## Exercise 03: Data Cleaning and Processing\n",
        "### 3.1 Working on the \"Cabin\" column\n",
        "Find unique entries in the Cabin column. We can label all passengers in two categories having a cabin or not. Check the data type(use: type) of each entry of the Cabin. Convert a string data type into '1' i.e. passengers with cabin and others into '0' i.e. passengers without cabin.  Write a function for the above operation and apply it to the cabin column and create another column with the name \" Has_cabin\" containing only 0 or 1 entries.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train['Cabin'].unique()\n",
        "x_train.dtypes\n",
        "ss=x_train['Cabin']\n",
        "ss.dtypes"
      ],
      "metadata": {
        "id": "KozjEOo9VS3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head()"
      ],
      "metadata": {
        "id": "nHC6cS-v1-ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_cabin(data):\n",
        "  Has_cabin = []\n",
        "  for i in data:\n",
        "    if str(i)=='nan':\n",
        "      Has_cabin.append(0)\n",
        "    else:\n",
        "      Has_cabin.append(1)\n",
        "  return Has_cabin\n"
      ],
      "metadata": {
        "id": "n9N82uXWWf81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Has_Cabin=has_cabin(x_train['Cabin'])\n",
        "x_train['Has_Cabin']=Has_Cabin\n",
        "x_train.shape"
      ],
      "metadata": {
        "id": "Sz9R9JX7QkA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 3.2 Working on \"SibSp\" & \"Parch\" columns\n",
        "Combine columns \"SibSp\" & \"Parch\" and create another column that represents the total passengers in one ticket with the name \"family_size\". In each ticket, there might be Siblings/Spouses (SibSp =Number of Siblings/Spouses Aboard) or Parents/Children (Parch=Number of Parents/Children Aboard ) along with the passenger who booked the ticket.\n",
        "\n"
      ],
      "metadata": {
        "id": "mUm20kyHVTZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Family_size = x_train['SibSp']+x_train['Parch']+1\n",
        "x_train['Family_size'] = Family_size"
      ],
      "metadata": {
        "id": "n2vHu13tWJx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "lEkyLqqEVX2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Working on the\"Embarked\" column\n",
        "The \"embarked\" column represents the port of Embarkation: Cherbourg(C), Queenstown(Q), and  Southampton(S ). Thus, the entries are of three categories in this column. Fill in the missing rows in this column. We can fill it with the most frequent category. Map these categorical string entries into numerical.\n",
        "\n"
      ],
      "metadata": {
        "id": "cGPgnKttVYRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train['Embarked'] = x_train['Embarked'].map({'C':2,'Q':1,'S':0})\n"
      ],
      "metadata": {
        "id": "IApw-rL1Y1jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=x_train.fillna(2)\n",
        "x_train['Embarked'].isna().sum()\n",
        "x_train.shape"
      ],
      "metadata": {
        "id": "OpOOEUPBtKM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.isna().sum()"
      ],
      "metadata": {
        "id": "rVTiJ27OVbzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Working on the \"Age\" column\n",
        "find the number of NaN entries in the age column and their row index. Calculate the mean, Standard deviation of the Age column and check the distribution of the age column.We can fill the missing values with randomly generated integer values between (mean+Standard deviation, mean-Standard deviation). Use : np.isnan; np.random.randint; concept of slicing dataframe. Convert the age column as an integer data type.\n",
        "\n"
      ],
      "metadata": {
        "id": "dWmjUnN3VcF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.loc[pd.isna(x_train[\"Age\"]), :].index"
      ],
      "metadata": {
        "id": "gbLqDB1hVf1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_of_age = x_train['Age'].mean()\n",
        "STD_of_age = x_train['Age'].std()\n",
        "size=x_train['Age'].isna().sum()\n",
        "#feature['Age'].isna().sum()"
      ],
      "metadata": {
        "id": "MFrug-3VakhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_arr = np.random.randint(low = mean_of_age-STD_of_age, high = mean_of_age+STD_of_age, size = 177)\n",
        "out_arr"
      ],
      "metadata": {
        "id": "a9a69SMpa6aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series = pd.Series(out_arr)\n",
        "x_train['Age'] = x_train['Age'].fillna(series)"
      ],
      "metadata": {
        "id": "ly2MwgJaobNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train['Age'] =x_train['Age'].fillna(mean_of_age)\n",
        "x_train['Age'].isna().sum()"
      ],
      "metadata": {
        "id": "WAt52TeGoqkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Working on \"sex\" column\n",
        "Map the Sex column as 'female' : 0, 'male': 1, and convert it into an integer data type.\n",
        "\n"
      ],
      "metadata": {
        "id": "doeanDr0VgGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train['Sex'] = x_train['Sex'].map({'male':1,'female':0})"
      ],
      "metadata": {
        "id": "dGN92EsEVlTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.8 Drop the columns\n",
        "\n",
        "Drop the columns: - \"PassengerId\", \"Name\",  \"SibSp\" & \"Parch\", \"Tickets\", \"Cabin\"\n",
        "\n",
        "Now apply different ML algorithms and check the accuracy of your model.\n",
        "\n"
      ],
      "metadata": {
        "id": "6oJI0bQOVpP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.drop([\"PassengerId\", \"Name\", \"SibSp\",\"Parch\", \"Ticket\", \"Cabin\"],axis=1)"
      ],
      "metadata": {
        "id": "1ZlnDtiiVvif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.9 Apply Standard Scalar"
      ],
      "metadata": {
        "id": "S2EfoVojebWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "STD = StandardScaler()\n",
        "STD.fit(x_train)\n",
        "x_train = STD.fit_transform(x_train)"
      ],
      "metadata": {
        "id": "fVWd4PEaeiod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.10 Create a single function for preprocessing the test set (X_test) and apply it.\n",
        "#### **Note**: All the pre-processing steps that were applied on the train set before ML Modelling are also applied on the test set before passing through the predict function."
      ],
      "metadata": {
        "id": "Kwa6Ua9Qgbi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(feature,label,test_size=0.33,random_state=42)"
      ],
      "metadata": {
        "id": "uFKNQoux-Kq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a function\n",
        "\n",
        "\n",
        "def has_cabin1(data):\n",
        "    Has_cabin = []\n",
        "\n",
        "    for i in data['Cabin']:\n",
        "        if str(i) == 'nan':\n",
        "            Has_cabin.append(0)\n",
        "        else:\n",
        "            Has_cabin.append(1)\n",
        "\n",
        "    data['Has_Cabin'] = Has_cabin\n",
        "    return Family(data)\n",
        "\n",
        "def Family(data_f):\n",
        "    Family_size = data_f['SibSp']+data_f['Parch']\n",
        "    data_f['Family_size'] = Family_size\n",
        "    return Embark(data_f)\n",
        "\n",
        "def Embark(data_e):\n",
        "    data_e['Embarked'] = data_e['Embarked'].map({'C':2,'Q':1,'S':0})\n",
        "    data_e=data_e.fillna(2)\n",
        "    return Age(data_e)\n",
        "\n",
        "def Age(data_a):\n",
        "    mean_of_age = data_a['Age'].mean()\n",
        "    STD_of_age = data_a['Age'].std()\n",
        "    size1=data_a['Age'].isna().sum()\n",
        "    out_arr = np.random.randint(low = mean_of_age-STD_of_age, high = mean_of_age+STD_of_age, size = size1)\n",
        "    data_a.loc[data_a['Age'].isna(),'Age']=out_arr\n",
        "    data_a['Age']=data_a['Age'].astype(int)\n",
        "    return Sex(data_a)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Sex(data_s):\n",
        "    data_s['Sex'] = data_s['Sex'].map({'male':1,'female':0})\n",
        "    data_s = data_s.drop([\"PassengerId\", \"Name\", \"SibSp\",\"Parch\", \"Ticket\", \"Cabin\"],axis=1)\n",
        "    return Scale(data_s)\n",
        "\n",
        "\n",
        "\n",
        "def Scale(data_sc):\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    STD = StandardScaler()\n",
        "    STD.fit(data_sc)\n",
        "    data_sc = STD.fit_transform(data_sc)\n",
        "    return data_sc\n"
      ],
      "metadata": {
        "id": "RLFNNM0SgqZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Applyting above function\n",
        "x_test=has_cabin1(x_test)\n",
        "x_train=has_cabin1(x_train)\n",
        "x_train.shape,y_train.shape"
      ],
      "metadata": {
        "id": "urGx3SRcc0kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise  4. Apply Multiple ML Algo. along with  Ensemble Technique (Voting classifier) and display the accuracy\n",
        "#### Expected Accuracy >= 80%\n"
      ],
      "metadata": {
        "id": "zUMFQj-Gc1BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR=LogisticRegression()\n",
        "LR.fit(x_train,y_train)\n",
        "LR.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "Xd-21QgEc2TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(max_depth=8, random_state=0)\n",
        "clf.fit(x_train, y_train)\n",
        "clf.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "GIHZRO5a7hJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "DTC = DecisionTreeClassifier(random_state=0, max_depth=6)\n",
        "DTC.fit(x_train,y_train)\n",
        "DTC.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "nk8Ab8fq8FkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "BG = BaggingClassifier()\n",
        "BG.fit(x_train,y_train)\n",
        "BG.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "12anyYQ5Bqho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "clf1 = svm.SVC()\n",
        "clf1.fit(x_train, y_train)\n",
        "clf1.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "rTFVkTlpaxkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "voting = VotingClassifier(estimators=[ ('SVC', clf1),('LR', LR), ('RF', clf)],voting='hard')\n",
        "\n",
        "voting.fit(x_train, y_train)\n",
        "voting.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "4iSNQy02aiE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise  5. Pre-process the test_set for Kaggle Submission\n",
        "Again we have to apply the same preprocess function and standard scaler on this test set before passing through predict function.\n",
        "\n",
        "#### Understanding the test set:"
      ],
      "metadata": {
        "id": "NIf7BgedjLZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ly8tLrULwDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle_data = pd.read_csv('test_titanic.csv')\n",
        "PassengerId=kaggle_data['PassengerId']\n",
        "kaggle_data\n"
      ],
      "metadata": {
        "id": "A4ApkkLec2V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: In the initial train set there were no missing entries in the \"Fare\" column. But, now for the submission test set, there is one missing entry in this column.\n",
        "\n",
        "#### There will be a minor change in the preprocess function to address the above issue."
      ],
      "metadata": {
        "id": "syRBMp7ilrbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def has_cabin2(data):\n",
        "    Has_cabin = []\n",
        "\n",
        "    for i in data['Cabin']:\n",
        "        if str(i) == 'nan':\n",
        "            Has_cabin.append(0)\n",
        "        else:\n",
        "            Has_cabin.append(1)\n",
        "\n",
        "    data['Has_Cabin'] = Has_cabin\n",
        "    return Family(data)\n",
        "\n",
        "def Family(data_f):\n",
        "    Family_size = data_f['SibSp']+data_f['Parch']\n",
        "    data_f['Family_size'] = Family_size\n",
        "    return Embark(data_f)\n",
        "\n",
        "def Embark(data_e):\n",
        "    data_e['Embarked'] = data_e['Embarked'].map({'C':2,'Q':1,'S':0})\n",
        "    data_e=data_e.fillna(2)\n",
        "    return Age(data_e)\n",
        "\n",
        "def Age(data_a):\n",
        "    mean_of_age = data_a['Age'].mean()\n",
        "    STD_of_age = data_a['Age'].std()\n",
        "    size1=data_a['Age'].isna().sum()\n",
        "    out_arr = np.random.randint(low = mean_of_age-STD_of_age, high = mean_of_age+STD_of_age, size = size1)\n",
        "    data_a.loc[data_a['Age'].isna(),'Age']=out_arr\n",
        "    data_a['Age']=data_a['Age'].astype(int)\n",
        "    return Sex(data_a)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Sex(data_s):\n",
        "    data_s['Sex'] = data_s['Sex'].map({'male':1,'female':0})\n",
        "    data_s = data_s.drop([\"PassengerId\", \"Name\", \"SibSp\",\"Parch\", \"Ticket\", \"Cabin\"],axis=1)\n",
        "    return data_s"
      ],
      "metadata": {
        "id": "-FWdMhuiLyEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle = has_cabin2(kaggle_data)\n",
        "kaggle['Fare']=kaggle['Fare'].fillna(value=kaggle['Fare'].mean())\n"
      ],
      "metadata": {
        "id": "Ppk5Fq0olrGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle.isna().sum()"
      ],
      "metadata": {
        "id": "PnWdAMnvgIdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "STD = StandardScaler()\n",
        "STD.fit(kaggle)\n",
        "kaggle = STD.fit_transform(kaggle)\n"
      ],
      "metadata": {
        "id": "hQ4Lsp6znhrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise  6. Prediction for test data for submission"
      ],
      "metadata": {
        "id": "c-zATg3NnlKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = voting.predict(kaggle)\n",
        "prediction"
      ],
      "metadata": {
        "id": "bvDpq5EHnkcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise  7. Saving the CSV file for submission\n",
        "Create a CSV file containing the first column as \"PassengerID\" of the test_sub file and \"Survived\" as the second column which stores the prediction of the test_sub file."
      ],
      "metadata": {
        "id": "fSYItZ_wn_1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Prediction = pd.DataFrame(prediction,columns=['Survived'])\n",
        "Final_result =pd.concat([PassengerId,Prediction],axis=1)\n",
        "Final_result"
      ],
      "metadata": {
        "id": "iAL0P5R-n2MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OijBZpMunIuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}